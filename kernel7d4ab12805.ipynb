{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom itertools import product\nimport sklearn\nimport scipy.sparse \nimport lightgbm \nimport gc\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = pd.read_csv('../input/sales_train_v2.csv')\nprint('sales_train')\ndisplay(sales_train.head())\n\ntest = pd.read_csv('../input/test.csv')\nprint('test')\ndisplay(test.head())\n\nitems = pd.read_csv('../input/items.csv')\nprint('items')\ndisplay(items.head())\n\nitem_categories = pd.read_csv('../input/item_categories.csv')\nprint('item_categories')\ndisplay(item_categories.head())\n\nshops = pd.read_csv('../input/shops.csv')\nprint('shops')\ndisplay(shops.head())\n\nsample_submission = pd.read_csv('../input/sample_submission.csv')\nprint('sample_submission')\ndisplay(sample_submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('sales_train')\ndisplay(sales_train.describe(include='all').T)\n\nprint('test')\ndisplay(test.describe(include='all').T)\n\nprint('items')\ndisplay(items.describe(include='all').T)\n\nprint('item_categories')\ndisplay(item_categories.describe(include='all').T)\n\nprint('shops')\ndisplay(shops.describe(include='all').T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['item_price','item_cnt_day']:\n    plt.figure()\n    plt.title(col)\n    sns.boxplot(x=sales_train[col]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing outliers over 99 %\n\nshape0 = sales_train.shape[0] # train size before dropping values\nfor col in ['item_price','item_cnt_day']:\n    max_val = sales_train[col].quantile(.99) # get 99th percentile value\n    sales_train = sales_train[sales_train[col]<max_val] # drop outliers\n    print(f'{shape0-sales_train.shape[0]} {col} values over {max_val} removed')\n\nprint(f'new training set has {sales_train.shape[0]} records')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train[sales_train['item_price']<=0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train=sales_train[sales_train['item_price']>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature matrix creation\n\n# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train.loc[sales_train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales_train.loc[sales_train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid.shape)\ngrid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# appending test data to matrix with next month's date_block_num\n# test file predicts next month\n\n# latest month\ngrid['date_block_num'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# append next month\ntest['date_block_num'] = 34\n# add to grid\ngrid = pd.concat([grid, test[grid.columns]], ignore_index=True)\nprint('grid shape: ',grid.shape)\nprint('missing values:',grid.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features creation\n# creating monthly features from sales_train  and features aggregatd to monthly level\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# Groupby data to get shop-item-month aggregates\ngb = sales_train.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum','trips':'size'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = sales_train.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum','trips_shop':'size'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = sales_train.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum','trips_item':'size'}})\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# median item monthly price (using median to avoid outliers)\ngb = sales_train.groupby(['date_block_num','item_id'],as_index=False).agg({'item_price':{'median_item_price':'median'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(all_data, gb, how='left', on=['date_block_num','item_id'])\n\n# make sure no na values\nprint('na median_item_price:',all_data['median_item_price'].isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first item appearance feature\ngb = all_data.groupby(['item_id'],as_index=False).agg({'date_block_num':{'item_first_month':'min'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(all_data, gb, how='left', on=['item_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use item_first_month to create new_item feature\nall_data['new_item'] = (all_data['date_block_num'] == all_data['item_first_month'])\nall_data['new_item'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\ndel grid, gb \ngc.collect();\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lag features from [1,2,3,4,5,12] months ago\n\n# List of columns that we will use to create lags\n\ncols_to_rename = list(all_data.columns.difference(index_cols+['item_first_month'])) \n\nshift_range = [1, 2, 3, 4, 5, 12]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\nall_data = downcast_dtypes(all_data)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean encoding for item_id\n\nitem_target_enc_na = .3343 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id')['target'].cumcount()\n\nall_data['item_target_enc'] = cumsum/cumcnt\nall_data['item_target_enc'].fillna(item_target_enc_na,inplace=True)\ncorr = np.corrcoef(all_data['target'].values, all_data['item_target_enc'])[0][1]\nprint(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean encoding for shop_id\n\nitem_target_enc_na = .3343 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('shop_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('shop_id')['target'].cumcount()\n\nall_data['shop_id_enc'] = cumsum/cumcnt\nall_data['shop_id_enc'].fillna(item_target_enc_na,inplace=True)\ncorr = np.corrcoef(all_data['target'].values, all_data['shop_id_enc'])[0][1]\nprint(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# not much corr\nall_data.drop(columns='shop_id_enc',inplace=True)\nall_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding month feature\n# assuming month of year plays a big role in number of items sold (seasonality). let's add month\n\n# figure out difference between month and month\nsales_train[['date','date_block_num']].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rule seems to be date_block_num%12+1\nsales_train['month'] = sales_train['date_block_num']%12+1\nsales_train[['date','month','date_block_num']].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add this to all_data\nall_data['month'] = all_data['date_block_num']%12+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items.groupby('item_category_id',as_index=False)['item_id'].count().rename(columns={'item_id':'total_items'}).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lots of items in each category (median of 43). we can use it as a categorical feature as well as encoded feature\n\nall_data['item_category_id'] = all_data['item_id'].map(items.set_index('item_id')['item_category_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['item_category_id'].isna().sum() # no missing categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if encoding item category is beneficial\n\nitem_target_enc_na = 0 # default na replacement\n# Expanding Mean\ncumsum = all_data.groupby('item_category_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_category_id')['target'].cumcount()\n\nitem_category_id_enc = cumsum/cumcnt\nitem_category_id_enc.fillna(item_target_enc_na,inplace=True)\nall_data['item_category_id_enc'] = item_category_id_enc\ncorr = np.corrcoef(all_data['target'].values, item_category_id_enc)[0][1]\nprint(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring Shop names\n\nwords = ' '.join(shops['shop_name']).split(' ')\nfrom collections import Counter\nc = Counter(words)\nc.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_by_store = sales_train.groupby('shop_id',as_index=False)['item_cnt_day'].sum()\nshop_by_store = shop_by_store.merge(shops, on='shop_id')\nprint(shop_by_store['shop_name'].isna().sum())\nshop_by_store.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_by_store['name_array'] = shop_by_store['shop_name'].str.split(' ')\ntop_words = [x for x,y in c.most_common(6)] # common words in shop name\nfor w in top_words:\n    shop_by_store[w] = shop_by_store['shop_name'].map(lambda x: 1 if w in x else 0)\n#     shop_by_store[w] = w in shop_by_store['shop_name'].str.split(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for w in top_words:\n    print(shop_by_store.groupby(by=w)['item_cnt_day'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_words = top_words[0:4] # important shop features\nshops['name_array'] = shops['shop_name'].str.split(' ')\nfor w in top_words:\n    shops[w] = shops['shop_name'].map(lambda x: 1 if w in x else 0)\n\nall_data = pd.merge(all_data,shops[['shop_id']+top_words],on='shop_id',how='left') # merge\n\nprint(all_data[top_words].isna().sum()) # make sure no nulls\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train/Validation/Test Split\n\nleaking_columns = ['median_item_price','date_block_num','target','target_shop','target_item','trips','trips_shop','trips_item']\n\nX_train = all_data.loc[all_data['date_block_num'] < 33].drop(leaking_columns, axis=1)\nX_val = all_data.loc[all_data['date_block_num'] == 33].drop(leaking_columns, axis=1)\nX_test = all_data.loc[all_data['date_block_num'] == 34].drop(leaking_columns, axis=1)\n\ny_train = all_data.loc[all_data['date_block_num'] < 33,'target'].values\ny_val = all_data.loc[all_data['date_block_num'] == 33,'target'].values\n\n# save all_data\n# all_data.to_csv('all_data.csv',index=False)\ndel all_data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation_function(y_pred,y_true):\n    print(f'rmse before [0,20] clipping: {mean_squared_error(y_true, y_pred)}')\n    y_pred = y_pred.clip(0,20)\n    y_true = y_true.clip(0,20)\n    print(f'rmse after [0,20] clipping: {mean_squared_error(y_true, y_pred)}')\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear regression\n\nlr = LinearRegression()\nlr.fit(X_train.values, y_train)\n# (due to memory issues we train on half the data)\n# lr.fit(X_train[round(X_train.shape[0]/2):-1].values, y_train[round(X_train.shape[0]/2):-1])\npred_lr = lr.predict(X_val.values)\n\nprint('Test R-squared for linreg is %f' % r2_score(y_val, pred_lr))\nvalidation_function(y_val,pred_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n# specify your initial configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'root_mean_squared_error'},\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 10\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=15,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                early_stopping_rounds=5)\n\nprint(gbm.pandas_categorical)\nlgb.plot_importance(gbm,figsize=(10,10));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update parameters\n\ncategorical_features = ['shop_id','item_id','item_category_id','new_item']+top_words\n\nlgb_train = lgb.Dataset(X_train, y_train,categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,categorical_feature=categorical_features)\n\ngbm2 = lgb.train(params,\n                lgb_train,\n                num_boost_round=15,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                categorical_feature = categorical_features,\n                early_stopping_rounds=5)\n\nprint(gbm2.pandas_categorical)\nlgb.plot_importance(gbm2,figsize=(10,10));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, y_train,categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,categorical_feature=categorical_features)\n\nlgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':10 \n              }\n\nmodel = lgb.train(lgb_params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=[lgb_train,lgb_eval],\n                valid_names=['train','val'],\n                categorical_feature = categorical_features,\n                early_stopping_rounds=5)\n\n\nprint(model.pandas_categorical)\nlgb.plot_importance(model,figsize=(10,10));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb = model.predict(X_val)\n\nprint('Test R-squared for LightGBM is %f' % r2_score(y_val, pred_lgb))\nvalidation_function(y_val,pred_lgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying Ensemble\n\n# train\nX_train_level2 = np.c_[model.predict(X_train), lr.predict(X_train.values)] \nlr2 = LinearRegression()\nlr2.fit(X_train_level2, y_train)\n\n# predict\nX_val_level2 = np.c_[model.predict(X_val), lr.predict(X_val.values)] \npred_lr2 = lr2.predict(X_val_level2)\n\nvalidation_function(y_val,pred_lr2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submitting results on Test set\n\n# lightgbm and lr predicitons\npred_lgb = model.predict(X_test).clip(0,20)\npred_lr = lr.predict(X_test.values).clip(0,20)\n\n# ensamble predicitons\nX_test_level2 = np.c_[pred_lgb, pred_lr] \npred_ensamble = lr2.predict(X_test_level2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make sure results are in the same order as the original test set\n(test[['shop_id','item_id']].values == X_test[['shop_id','item_id']].values).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data={'ID':test['ID'],'item_cnt_month':pred_lgb}).to_csv('lgbm_predictions.csv',index=False)\npd.DataFrame(data={'ID':test['ID'],'item_cnt_month':pred_ensamble}).to_csv('ensamble_predictions.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}